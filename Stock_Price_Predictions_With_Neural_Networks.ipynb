{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d41bb96e",
   "metadata": {},
   "source": [
    "<h2> Importing Packages </h2>\n",
    "Numpy for scientific computation <br>\n",
    "Pandas for accessing the csv file with data <br>\n",
    "sklearn.model_selection for selecting train, dev, test sets in a re producable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1505,
   "id": "48cb643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cca2b1a",
   "metadata": {},
   "source": [
    "<h2> Preparing Train, Dev and Test Sets </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1506,
   "id": "f37568c9",
   "metadata": {},
   "outputs": [],
   "source": [
    " ## Load Data\n",
    "df = pd.read_csv('Data/financial_data2.csv')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1507,
   "id": "0c3180e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting into X and Y\n",
    "X = df.drop('Price', axis=1)\n",
    "Y = df['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1508,
   "id": "ccf66048",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First Split 60% for train and 40% for not train\n",
    "X_train, X_not_train, Y_train, Y_not_train = train_test_split(X, Y, test_size=0.4, random_state=42)\n",
    "## Second Split Allocating the 40% Remaining data equally between Test and Dev sets (20% each)\n",
    "X_dev, X_test, Y_dev, Y_test = train_test_split(X_not_train, Y_not_train, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1509,
   "id": "4ae8ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_dev = scaler.transform(X_dev)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2b71d5",
   "metadata": {},
   "source": [
    "<h2> Helper Function Development <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1510,
   "id": "5d2fd307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z): ## Sigmoid Function\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1511,
   "id": "35747d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): ## ReLU (Rectified Linear Unit) Function\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1512,
   "id": "c6c49c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h1, n_h2, n_y): ## Inititalize the \"W\" Parameters to avoid the resulting linear function if all the same originally\n",
    "    np.random.seed(1)\n",
    "    W1 = np.random.randn(n_h1, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h1, 1))\n",
    "    W2 = np.random.randn(n_h2, n_h1) * 0.01\n",
    "    b2 = np.zeros((n_h2, 1))\n",
    "    W3 = np.random.randn(n_y, n_h2) * 0.01\n",
    "    b3 = np.zeros((n_y, 1))\n",
    "\n",
    "    parameters = {\"W1\": W1, \"b1\": b1,\n",
    "                  \"W2\": W2, \"b2\": b2,\n",
    "                  \"W3\": W3, \"b3\": b3}\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1513,
   "id": "cd142de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = Z3  # No activation for the last layer\n",
    "\n",
    "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "\n",
    "    return A3, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1514,
   "id": "3d0be790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A3, Y): # Mean Squared Error\n",
    "    m = Y.shape[1]\n",
    "    cost = (1 / (2 * m)) * np.sum(np.square(A3 - Y)) \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1515,
   "id": "bd7cce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X, Y, cache):\n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "\n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1./m * np.dot(dZ3, A2.T)\n",
    "    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n",
    "\n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1./m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
    "\n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1./m * np.dot(dZ1, X.T)\n",
    "    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n",
    "\n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1516,
   "id": "fb468c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    parameters[\"W1\"] = parameters[\"W1\"] - learning_rate * grads[\"dW1\"]\n",
    "    parameters[\"b1\"] = parameters[\"b1\"] - learning_rate * grads[\"db1\"]\n",
    "    parameters[\"W2\"] = parameters[\"W2\"] - learning_rate * grads[\"dW2\"]\n",
    "    parameters[\"b2\"] = parameters[\"b2\"] - learning_rate * grads[\"db2\"]\n",
    "    parameters[\"W3\"] = parameters[\"W3\"] - learning_rate * grads[\"dW3\"]\n",
    "    parameters[\"b3\"] = parameters[\"b3\"] - learning_rate * grads[\"db3\"]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1517,
   "id": "986429a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters) :\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
    "    for l in range(1, L + 1):\n",
    "        v[\"dW\" + str(l)] = np.zeros_like(parameters[\"W\" + str(l)])\n",
    "        v[\"db\" + str(l)] = np.zeros_like(parameters[\"b\" + str(l)])\n",
    "        s[\"dW\" + str(l)] = np.zeros_like(parameters[\"W\" + str(l)])\n",
    "        s[\"db\" + str(l)] = np.zeros_like(parameters[\"b\" + str(l)])\n",
    "    \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1518,
   "id": "32dc6729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "    L = len(parameters) // 2\n",
    "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
    "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
    "    \n",
    "    # Perform Adam update on all parameters\n",
    "    for l in range(1, L + 1):\n",
    "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
    "        v[\"dW\" + str(l)] = beta1 * v[\"dW\" + str(l)] + (1 - beta1) * grads['dW' + str(l)]\n",
    "        v[\"db\" + str(l)] = beta1 * v[\"db\" + str(l)] + (1 - beta1) * grads['db' + str(l)]\n",
    "\n",
    "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
    "        v_corrected[\"dW\" + str(l)] = v[\"dW\" + str(l)] / (1 - np.power(beta1, t))\n",
    "        v_corrected[\"db\" + str(l)] = v[\"db\" + str(l)] / (1 - np.power(beta1, t))\n",
    "\n",
    "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
    "        s[\"dW\" + str(l)] = beta2 * s[\"dW\" + str(l)] + (1 - beta2) * np.power(grads['dW' + str(l)], 2)\n",
    "        s[\"db\" + str(l)] = beta2 * s[\"db\" + str(l)] + (1 - beta2) * np.power(grads['db' + str(l)], 2)\n",
    "\n",
    "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
    "        s_corrected[\"dW\" + str(l)] = s[\"dW\" + str(l)] / (1 - np.power(beta2, t))\n",
    "        s_corrected[\"db\" + str(l)] = s[\"db\" + str(l)] / (1 - np.power(beta2, t))\n",
    "\n",
    "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
    "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * v_corrected[\"dW\" + str(l)] / (np.sqrt(s_corrected[\"dW\" + str(l)]) + epsilon)\n",
    "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * v_corrected[\"db\" + str(l)] / (np.sqrt(s_corrected[\"db\" + str(l)]) + epsilon)\n",
    "\n",
    "    return parameters, v, s, v_corrected, s_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1519,
   "id": "973aa679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size=2048, seed=0):\n",
    "    \n",
    "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1, m))\n",
    "\n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k+1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k+1) * mini_batch_size]\n",
    "\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # For handling the end case (last mini-batch < mini_batch_size i.e less than 64)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size :]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size :]\n",
    "\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1520,
   "id": "8e54609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lr(learning_rate0, epoch_num, decay_rate):\n",
    "    learning_rate = (1/(1+(decay_rate*epoch_num)))*learning_rate0\n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1521,
   "id": "ea961537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_lr_decay(learning_rate0, epoch_num, decay_rate, time_interval=1000):\n",
    "    learning_rate = (1/(1+decay_rate*np.floor(epoch_num/time_interval)))*learning_rate0\n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1522,
   "id": "848721f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1(X, Y, learning_rate = 0.00001, num_iterations = 350001): ## Without Adam and Batch Gradient Descent\n",
    "    grads = {}\n",
    "    costs = [] \n",
    "    m = X.shape[1] \n",
    "    layers_dims = [X.shape[0], 20, 7, 1]\n",
    "\n",
    "    parameters = initialize_parameters(layers_dims[0], layers_dims[1], layers_dims[2], layers_dims[3])\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "        A3, cache = forward_propagation(X, parameters)\n",
    "        cost = compute_cost(A3, Y)\n",
    "        grads = backward_propagation(X, Y, cache)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            if i % 1000 == 0:\n",
    "                print(\"Cost after iteration Model1 {}: {}\".format(i, cost))\n",
    "            costs.append(cost)\n",
    "\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per thousands)')\n",
    "    plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1523,
   "id": "476a2486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2(X, Y, learning_rate = 0.00001, num_iterations = 360001, beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8): ## With Adam Single Batch Gradient Decent\n",
    "    grads = {}\n",
    "    costs = [] \n",
    "    m = X.shape[1] \n",
    "    layers_dims = [X.shape[0], 20, 7, 1]\n",
    "\n",
    "    parameters = initialize_parameters(layers_dims[0], layers_dims[1], layers_dims[2], layers_dims[3])\n",
    "    \n",
    "    # Initialize v and s for Adam optimizer\n",
    "    v, s = initialize_adam(parameters)\n",
    "    t = 0\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        # Forward propagation\n",
    "        A3, cache = forward_propagation(X, parameters)\n",
    "\n",
    "        # Compute cost\n",
    "        cost = compute_cost(A3, Y)\n",
    "\n",
    "        # Backward propagation\n",
    "        grads = backward_propagation(X, Y, cache)\n",
    "\n",
    "        # Update parameters using Adam optimization\n",
    "        t = t + 1 # Increment the timestep 't'\n",
    "        parameters, v, s, v_corrected, s_corrected = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2, epsilon)\n",
    "\n",
    "        # Print the cost every 1000 iterations\n",
    "        if i % 100 == 0:\n",
    "            if i % 1000 == 0:\n",
    "                print(\"Cost after iteration Model2 {}: {}\".format(i, cost))\n",
    "            costs.append(cost)\n",
    "            \n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per thousands)')\n",
    "    plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "    plt.show()  \n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1524,
   "id": "e39a5467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model3(X, Y, learning_rate = 0.0000001, num_iterations = 1801, mini_batch_size = 2048, beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8): ## Mini Batch With Adam Opimizer\n",
    "    grads = {}\n",
    "    costs = [] \n",
    "    m = X.shape[1] \n",
    "    layers_dims = [X.shape[0], 20, 7, 1]\n",
    "\n",
    "    parameters = initialize_parameters(layers_dims[0], layers_dims[1], layers_dims[2], layers_dims[3])\n",
    "    # Initialize v and s for Adam optimizer\n",
    "    v, s = initialize_adam(parameters)\n",
    "    t = 0\n",
    "    for i in range(num_iterations):\n",
    "        # Create mini-batches\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size, seed = i)\n",
    "\n",
    "        for minibatch in minibatches:\n",
    "            # Select a minibatch\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "            # Forward propagation\n",
    "            A3, cache = forward_propagation(minibatch_X, parameters)\n",
    "\n",
    "            # Compute cost\n",
    "            cost = compute_cost(A3, minibatch_Y)\n",
    "\n",
    "            # Backward propagation\n",
    "            grads = backward_propagation(minibatch_X, minibatch_Y, cache)\n",
    "\n",
    "            # Update parameters using Adam optimization\n",
    "            t = t + 1 # Increment the timestep 't'\n",
    "            parameters, v, s, v_corrected, s_corrected = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2, epsilon)\n",
    "\n",
    "        # Print the cost every 1000 iterations\n",
    "        if i % 100 == 0:\n",
    "            if i % 1000 == 0:\n",
    "                print(\"Cost after iteration Model3 {}: {}\".format(i, cost))\n",
    "            costs.append(cost)\n",
    "    \n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per thousands)')\n",
    "    plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "    plt.show() \n",
    "    \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1525,
   "id": "da6bb4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model4(X, Y, learning_rate0 = 0.000001, num_iterations = 180001, mini_batch_size = 2048, decay_rate = 0.001, time_interval=1000, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8): ## Mini Batch With Adam Opimizer and Learning Rate Decay\n",
    "    grads = {}\n",
    "    costs = [] \n",
    "    m = X.shape[1] \n",
    "    layers_dims = [X.shape[0], 20, 7, 1]\n",
    "\n",
    "    parameters = initialize_parameters(layers_dims[0], layers_dims[1], layers_dims[2], layers_dims[3])\n",
    "    # Initialize v and s for Adam optimizer\n",
    "    v, s = initialize_adam(parameters)\n",
    "    t = 0\n",
    "    for i in range(num_iterations):\n",
    "        # Create mini-batches\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size, seed = i)\n",
    "\n",
    "        learning_rate = update_lr(learning_rate0, i, decay_rate) # update learning rate using exponential decay\n",
    "        \n",
    "        for minibatch in minibatches:\n",
    "            # Select a minibatch\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "            # Forward propagation\n",
    "            A3, cache = forward_propagation(minibatch_X, parameters)\n",
    "\n",
    "            # Compute cost\n",
    "            cost = compute_cost(A3, minibatch_Y)\n",
    "\n",
    "            # Backward propagation\n",
    "            grads = backward_propagation(minibatch_X, minibatch_Y, cache)\n",
    "\n",
    "            # Update parameters using Adam optimization\n",
    "            t = t + 1 # Increment the timestep 't'\n",
    "            parameters, v, s, v_corrected, s_corrected = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2, epsilon)\n",
    "\n",
    "        # Update the learning rate every time_interval epochs\n",
    "        if i % time_interval == 0:\n",
    "            learning_rate = schedule_lr_decay(learning_rate0, i, decay_rate, time_interval)\n",
    "\n",
    "        # Print the cost every 1000 iterations\n",
    "        if i % 100 == 0:\n",
    "            if i % 1000 == 0:\n",
    "                print(\"Cost after iteration Model4 {}: {}\".format(i, cost))\n",
    "            costs.append(cost)\n",
    "            \n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per thousands)')\n",
    "    plt.title(\"Learning rate = \" + str(learning_rate0))\n",
    "    plt.show()   \n",
    "    \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1526,
   "id": "be39f61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train.values.T\n",
    "X_train = X_train.T\n",
    "Y_train = Y_train.values.reshape(1, -1)\n",
    "\n",
    "# Run the model\n",
    "# parameters1 = model1(X_train, Y_train)\n",
    "# parameters2 = model2(X_train, Y_train)\n",
    "# parameters3 = model3(X_train, Y_train)\n",
    "# parameters4 = model4(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5d29e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration Model1 0: 14982.102892863948\n",
      "Cost after iteration Model1 1000: 14921.946967830294\n",
      "Cost after iteration Model1 2000: 14862.94917206602\n",
      "Cost after iteration Model1 3000: 14804.962986180763\n",
      "Cost after iteration Model1 4000: 14747.426694343496\n",
      "Cost after iteration Model1 5000: 14687.960997495522\n",
      "Cost after iteration Model1 6000: 14616.065336666361\n",
      "Cost after iteration Model1 7000: 14447.89419210902\n",
      "Cost after iteration Model1 8000: 505.48752963764355\n",
      "Cost after iteration Model1 9000: 300.0855904732903\n",
      "Cost after iteration Model1 10000: 284.16139140204865\n",
      "Cost after iteration Model1 11000: 280.3139895724089\n",
      "Cost after iteration Model1 12000: 279.34004502733296\n",
      "Cost after iteration Model1 13000: 279.0056699860744\n",
      "Cost after iteration Model1 14000: 278.82709062191566\n",
      "Cost after iteration Model1 15000: 278.69953886021244\n",
      "Cost after iteration Model1 16000: 278.5909898042847\n",
      "Cost after iteration Model1 17000: 278.49309348502135\n",
      "Cost after iteration Model1 18000: 278.4050829044002\n",
      "Cost after iteration Model1 19000: 278.3251403265621\n",
      "Cost after iteration Model1 20000: 278.2515986894151\n",
      "Cost after iteration Model1 21000: 278.18362866780643\n",
      "Cost after iteration Model1 22000: 278.12064814645396\n",
      "Cost after iteration Model1 23000: 278.0619548030135\n",
      "Cost after iteration Model1 24000: 278.0070249588546\n",
      "Cost after iteration Model1 25000: 277.9555024368095\n",
      "Cost after iteration Model1 26000: 277.90743517043575\n",
      "Cost after iteration Model1 27000: 277.8625025895285\n",
      "Cost after iteration Model1 28000: 277.8202614032152\n",
      "Cost after iteration Model1 29000: 277.7801380054153\n",
      "Cost after iteration Model1 30000: 277.74237352384796\n",
      "Cost after iteration Model1 31000: 277.70666562686415\n",
      "Cost after iteration Model1 32000: 277.6729141500732\n",
      "Cost after iteration Model1 33000: 277.64094367335736\n",
      "Cost after iteration Model1 34000: 277.6104588757473\n",
      "Cost after iteration Model1 35000: 277.5814434723872\n",
      "Cost after iteration Model1 36000: 277.5538018685277\n",
      "Cost after iteration Model1 37000: 277.5275254227895\n",
      "Cost after iteration Model1 38000: 277.50250308262605\n",
      "Cost after iteration Model1 39000: 277.4784739081527\n"
     ]
    }
   ],
   "source": [
    "parameters1 = model1(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914da6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters2 = model2(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8419d6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters3 = model3(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac33e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters4 = model4(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8c1f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    A3, cache = forward_propagation(X, parameters)\n",
    "    \n",
    "    return A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a86551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_dev = X_dev.values.T\n",
    "Y_dev = Y_dev.values.reshape(1, -1)\n",
    "X_dev = X_dev.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a7e434",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dev1 = predict(X_dev, parameters1)\n",
    "predictions_dev2 = predict(X_dev, parameters2)\n",
    "predictions_dev3 = predict(X_dev, parameters3)\n",
    "predictions_dev4 = predict(X_dev, parameters4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c599f5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_cost1 = compute_cost(predictions_dev1, Y_dev)\n",
    "print(\"Cost on dev set Model1: {}\".format(dev_cost1))\n",
    "\n",
    "dev_cost2 = compute_cost(predictions_dev2, Y_dev)\n",
    "print(\"Cost on dev set Model2: {}\".format(dev_cost2))\n",
    "\n",
    "dev_cost3 = compute_cost(predictions_dev3, Y_dev)\n",
    "print(\"Cost on dev set Model3: {}\".format(dev_cost3))\n",
    "\n",
    "dev_cost4 = compute_cost(predictions_dev4, Y_dev)\n",
    "print(\"Cost on dev set Model4: {}\".format(dev_cost4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a9c0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(predictions_dev[0])):\n",
    "#     print(\"Predicted: \" + str(predictions_dev[0][i]) + \" Actual: \" + str(Y_dev.T[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef68965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = X_test.values.T\n",
    "# Y_test = Y_test.values.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cdb9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_test = predict(X_test, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b47e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_cost = compute_cost(predictions_test, Y_test)\n",
    "# print(\"Cost on test set: {}\".format(test_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859fde83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(predictions_test[0])):\n",
    "#     print(\"Predicted: \" + str(predictions_test[0][i]) + \" Actual: \" + str(Y_test.T[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
